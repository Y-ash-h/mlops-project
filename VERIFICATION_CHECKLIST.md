# MLOps System Verification Checklist

## ✅ System Status Overview

Based on the comprehensive evaluation provided, here's how to verify your MLOps system is working correctly.

---

## 1. Pipeline Integrity (Airflow)

### Check 1.1: Grid View Verification
**Location:** Airflow UI → Grid View (`http://localhost:8099`)

**Steps:**
1. Open the Grid View for your DAG (e.g., `ml_training_dag` or `mlops_dag`)
2. **Expected:** Right-most column should be entirely **green** (success)
3. **Expected:** One task should be **pink** (skipped) - this is your branching logic working correctly

**What Green Means:**
- ✅ Dark Green = Success
- ⚠️ Pink = Skipped (intentional - your branching logic)
- ❌ Red = Failure (click to debug)

### Check 1.2: Graph View Verification
**Location:** Airflow UI → Graph View

**Steps:**
1. View the DAG graph
2. **Expected:** See arrows showing task dependencies
3. **Expected:** Dark green borders around tasks that executed successfully
4. **Expected:** See the branch after `check_explainability`:
   - One path executed (green)
   - One path skipped (pink)

**What This Confirms:**
- Complex branching logic is working
- Pipeline intelligently decides when to explain models
- Resource optimization is working

### Check 1.3: Model Training Verification
**Steps:**
1. Click on `train_model` task in Grid View
2. Click "Logs" tab
3. Scroll to bottom of logs
4. **Look for:**
   - `INFO - Model saved to...` or
   - `INFO - Active run_id: ...` or
   - `INFO - Registered model version: ...`

**This confirms:** A model was actually produced and registered

---

## 2. Serving Health (FastAPI)

### Check 2.1: Container Status
```bash
docker ps | grep model_serving
```

**Expected Output:**
```
mlops_model_serving    Up X minutes    0.0.0.0:8000->8000/tcp
```

### Check 2.2: Health Endpoint
```bash
curl http://localhost:8000/health
```

**Success Response:**
```json
{
  "status": "healthy",
  "model_loaded": true
}
```

**If `model_loaded: false`:**
- Pipeline hasn't run yet, OR
- Model hasn't been saved to expected location

**If Connection Refused:**
- Container not running
- Run: `docker compose up -d model-serving`

---

## 3. End-to-End Prediction Test

### Check 3.1: Basic Prediction
```bash
curl -X POST "http://localhost:8000/predict" \
     -H "Content-Type: application/json" \
     -d '{"features": [{"feature1": 1.5, "feature2": 2.5}]}'
```

**Expected Response:**
```json
{
  "predictions": [0]
}
```

**This Proves:**
- ✅ Docker volume is working (FastAPI can read files Airflow created)
- ✅ Model libraries are compatible
- ✅ Feature engineering is working
- ✅ End-to-end pipeline is functional

### Check 3.2: Batch Prediction
```bash
curl -X POST "http://localhost:8000/predict" \
     -H "Content-Type: application/json" \
     -d '{"features": [
         {"feature1": 1.5, "feature2": 2.5},
         {"feature1": 3.0, "feature2": 4.0}
     ]}'
```

**Expected:** Array of predictions for each input

---

## 4. Monitoring Integration

### Check 4.1: Drift Detection Task
**Location:** Airflow Grid View

**Expected:**
- `monitor_data_drift` task is **green**
- This confirms Evidently AI is working in Docker

### Check 4.2: Monitoring Reports
```bash
ls -la mlops-pipeline/data/monitoring/
```

**Expected Files:**
- `drift_report.html` - Generated by Evidently AI
- Reports updated after each pipeline run

---

## 5. FastAPI Swagger UI (Interactive Testing)

### Access Swagger UI
**URL:** `http://localhost:8000/docs`

**What You'll See:**
- Interactive API documentation
- Two endpoints: `/health` and `/predict`
- "Try it out" buttons for each endpoint

### How to Use:
1. Click on `/predict` endpoint
2. Click "Try it out"
3. Paste your JSON in the request body:
   ```json
   {
     "features": [{"feature1": 1.5, "feature2": 2.5}]
   }
   ```
4. Click "Execute"
5. See the prediction response

**Benefits:**
- Test without writing code
- Share documentation with frontend developers
- Verify API contracts

---

## 6. MLflow Model Registry

### Check Model Registration
**Location:** MLflow UI (`http://localhost:5050`)

**Steps:**
1. Go to Models tab
2. Look for `tabular_model` (or your MODEL_REGISTRY_NAME)
3. **Expected:** See model versions
4. **Expected:** One version marked as "Production"

**This Confirms:**
- Models are being registered
- Model promotion is working
- Production model tracking is active

---

## Common Issues & Solutions

### Issue: `model_loaded: false` in health check
**Solution:**
1. Check if pipeline has run: `docker logs mlops_project_clean-airflow-1`
2. Verify model path exists: `ls -la mlops-pipeline/data/model/`
3. Ensure shared volume is mounted correctly in docker-compose.yml

### Issue: Connection refused on port 8000
**Solution:**
```bash
# Rebuild and start the service
docker compose build model-serving
docker compose up -d model-serving

# Check logs
docker logs mlops_model_serving
```

### Issue: Prediction returns error
**Possible Causes:**
- Model expects different features
- Feature engineering mismatch
- Model not compatible with input format

**Debug:**
```bash
# Check FastAPI logs
docker logs mlops_model_serving

# Verify model path
docker exec mlops_model_serving ls -la /opt/airflow/mlops-pipeline/data/model/
```

---

## Success Criteria Summary

✅ **All systems green when:**
1. Airflow Grid View shows all green (except intentional skips)
2. Health endpoint returns `model_loaded: true`
3. Prediction endpoint returns valid predictions
4. MLflow shows registered models
5. Monitoring reports are generated

**Congratulations!** Your MLOps pipeline is production-ready when all checks pass.

