# ğŸ¯ SIMPLE 3-STEP GUIDE TO RUN YOUR MLOPS PIPELINE

## âœ… CURRENT STATUS

**Services Running:** âœ“ All services are UP
**Data Ready:** âœ“ 25 samples in data.csv
**Pipeline:** ğŸŸ¡ TRIGGERED - Running now!

---

## ğŸš€ WHAT TO DO NOW

### Option 1: Watch in Airflow UI (RECOMMENDED)

1. **Open Airflow**: http://localhost:8099
   - Login: `admin` / `admin`

2. **Find your DAG**: Click `mlops_full_pipeline`

3. **Watch the Grid View**: 
   - Tasks will change from gray â†’ yellow â†’ green
   - Takes ~2-3 minutes total
   - âœ… All green = SUCCESS!

4. **Click any task** to see logs and understand what it's doing

### Option 2: Watch in Terminal

```bash
# Check if running
docker exec mlops_project_clean-airflow-1 airflow dags list-runs -d mlops_full_pipeline | head -5

# See task status
docker exec mlops_project_clean-airflow-1 airflow tasks list mlops_full_pipeline -t
```

---

## ğŸ“Š VIEW RESULTS (After completion)

### Check Files Generated:

```bash
# All output files
ls -lh data/clean/
ls -lh data/features/
ls -lh data/*.csv
ls -lh data/monitoring/
ls -lh data/*.html
```

### Open Reports:

```bash
# Drift report (data quality)
open data/monitoring/drift_report.html

# Model card (documentation)
open data/model_card.html
```

### View in MLflow:

1. **Open MLflow**: http://localhost:5050
2. Click **"Experiments"** â†’ **"demo_experiment"**
3. See your training run with:
   - Metrics (RMSE)
   - Artifacts (model files, SHAP plots)
   - Parameters

4. Click **"Models"** â†’ **"tabular_model"**
   - See registered model versions
   - Check which is in "Production"

---

## ğŸ“ UNDERSTANDING THE PIPELINE

### What's Happening Right Now:

**The pipeline is processing your data through 11 steps:**

```
1. ğŸ“Š detect_data_type       â†’ "This is tabular data"
2. ğŸ“¥ ingest_data            â†’ Load data.csv
3. âœ… validate_data          â†’ Check schema & quality
4. ğŸ”§ preprocess_data        â†’ Clean + add features
                                Creates: feature_1_x_feature_2
5. ğŸ¯ train_model            â†’ Train Random Forest
                                Log to MLflow
6. ğŸ“ˆ monitor_data_drift     â†’ Check for data shifts
7. ğŸš¨ drift_alert_check      â†’ Alert if drift detected
8. ğŸ”€ check_explainability   â†’ "Tabular? â†’ explain it"
9. ğŸ” explain_model          â†’ Generate SHAP plots
                                Shows feature importance
10. ğŸ† promote_model         â†’ Compare RMSE scores
                                Promote if better
11. ğŸ“„ generate_model_card   â†’ Create documentation
```

### Your Simple Dataset:

**Input:** `data/raw/data.csv`
```
feature1, feature2 â†’ target
   1.5,     2.3    â†’   0
   2.1,     3.4    â†’   1
   ...25 total rows
```

**After Feature Engineering:**
```
feature1, feature2, feature_1_x_feature_2 â†’ target
   1.5,     2.3,          3.45           â†’   0
   2.1,     3.4,          7.14           â†’   1
```

**After Split:**
- Training: 20 samples â†’ train model
- Validation: 5 samples â†’ evaluate performance

**Model:** Random Forest Classifier
- Predicts: 0 or 1
- Evaluates: Using RMSE metric
- Logs: Everything to MLflow

---

## ğŸ¯ SUCCESS INDICATORS

### In Airflow (http://localhost:8099):

âœ… **ALL TASKS GREEN** = Pipeline succeeded!

```
[ğŸŸ¢] detect_data_type
[ğŸŸ¢] ingest_data
[ğŸŸ¢] validate_data
[ğŸŸ¢] preprocess_data
[ğŸŸ¢] train_model
[ğŸŸ¢] monitor_data_drift
[ğŸŸ¢] drift_alert_check
[ğŸŸ¢] check_explainability
[ğŸŸ¢] explain_model
[ğŸŸ¢] promote_model
[ğŸŸ¢] generate_model_card
```

### Files Created:

```bash
âœ… data/clean/data.csv                    # Cleaned data
âœ… data/features/data.csv                 # With feature_1_x_feature_2
âœ… data/train.csv                         # 20 samples
âœ… data/validation.csv                    # 5 samples
âœ… data/monitoring/drift_report.html      # Drift analysis
âœ… data/model_card.html                   # Model docs
```

### In MLflow (http://localhost:5050):

```
âœ… New run in "demo_experiment"
âœ… Metrics logged (RMSE)
âœ… Model artifacts saved
âœ… SHAP plots generated
âœ… Model "tabular_model" registered
âœ… Version promoted to "Production"
```

---

## ğŸ“¸ WHAT YOU'LL SEE

### Airflow Grid View Example:

```
Run Date: 2025-11-24 09:19:18

detect â†’ ingest â†’ validate â†’ preprocess â†’ train â†’ monitor â†’ alert â†’ explain â†’ promote â†’ card
[ğŸŸ¢]     [ğŸŸ¢]      [ğŸŸ¢]        [ğŸŸ¢]        [ğŸŸ¢]     [ğŸŸ¢]      [ğŸŸ¢]     [ğŸŸ¢]       [ğŸŸ¢]       [ğŸŸ¢]

Status: SUCCESS
Duration: 2m 34s
```

### MLflow Experiment View Example:

```
demo_experiment

Run Name: RandomForest_v1
Start Time: 2025-11-24 09:19:45
Status: FINISHED

Metrics:
  rmse: 0.245

Parameters:
  n_estimators: 100
  max_depth: None
  random_state: 42

Artifacts:
  ğŸ“ model/
  ğŸ“Š shap_summary.png
  ğŸ“„ shap_values.csv
  ğŸ“ training_features.txt
```

### SHAP Explanation Plot:

Shows which features matter most:

```
Feature Importance (SHAP values)

feature_1_x_feature_2  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  (most important)
feature2               â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
feature1               â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ

â†’ Interaction term is most predictive!
```

---

## ğŸ”„ TRY IT YOURSELF

### Experiment 1: Add More Data

```bash
# Edit the CSV
nano data/raw/data.csv

# Add more rows, then trigger again
docker exec mlops_project_clean-airflow-1 airflow dags trigger mlops_full_pipeline
```

### Experiment 2: Compare Model Versions

1. Trigger pipeline multiple times
2. In MLflow, select multiple runs
3. Click "Compare" button
4. See which performed best!

### Experiment 3: Modify Features

Edit: `mlops-pipeline/src/data_preprocessing/feature_engineering.py`

Add new interaction terms:
```python
# Add this in feature engineering
df['feature1_squared'] = df['feature1'] ** 2
df['feature2_squared'] = df['feature2'] ** 2
```

Then trigger pipeline again!

---

## ğŸ“ LEARNING POINTS

### Key Concepts You're Seeing:

1. **ML Pipeline Automation**: 
   - Data flows automatically through all stages
   - No manual intervention needed

2. **Experiment Tracking (MLflow)**:
   - Every run is logged
   - Can compare performance over time
   - Reproducibility built-in

3. **Model Registry**:
   - Models are versioned
   - Promotion logic (Staging â†’ Production)
   - Easy rollback if needed

4. **Data Monitoring**:
   - Detect when data distribution changes
   - Alert before model degrades

5. **Explainability**:
   - Understand WHY model makes predictions
   - SHAP shows feature contributions

6. **Documentation**:
   - Model cards auto-generated
   - Includes metrics, features, intended use

---

## ğŸ’¡ QUICK REFERENCE

### Important URLs:
- **Airflow**: http://localhost:8099 (admin/admin)
- **MLflow**: http://localhost:5050
- **API**: http://localhost:8000

### Key Directories:
- **Input**: `data/raw/data.csv`
- **Outputs**: `data/clean/`, `data/features/`
- **Splits**: `data/train.csv`, `data/validation.csv`
- **Reports**: `data/monitoring/`, `data/*.html`

### Useful Commands:
```bash
# Trigger pipeline
docker exec mlops_project_clean-airflow-1 airflow dags trigger mlops_full_pipeline

# Check status
docker exec mlops_project_clean-airflow-1 airflow dags list-runs -d mlops_full_pipeline

# View logs
docker-compose logs airflow | tail -50

# Restart services
docker-compose restart
```

---

## âœ… CHECKLIST

After pipeline completes:

- [ ] All Airflow tasks show green
- [ ] New MLflow run visible in demo_experiment
- [ ] Model "tabular_model" in Model Registry
- [ ] drift_report.html exists and opens
- [ ] model_card.html exists and opens
- [ ] Can see SHAP plots in MLflow artifacts

---

## ğŸ‰ YOU'RE DONE!

**You've successfully run a complete MLOps pipeline!**

**What you accomplished:**
âœ… Automated data preprocessing
âœ… ML model training with tracking
âœ… Data quality monitoring
âœ… Model explainability
âœ… Automated model promotion
âœ… Documentation generation

**Next steps:**
1. Explore the Airflow and MLflow UIs
2. Read the detailed `QUICKSTART_GUIDE.md`
3. Try modifying the data or features
4. Compare multiple runs

**Need help?** Check `QUICKSTART_GUIDE.md` for detailed explanations!

---

ğŸš€ **Happy ML Engineering!**
